import sys
import os
import math
import re
from PySide6.QtCore import QThread, Signal

# --- æ ¸å¿ƒä¿®å¤ï¼šé˜²æ­¢ PyInstaller --noconsole æ¨¡å¼ä¸‹ transformers æŠ¥é”™ ---
class NullWriter:
    def write(self, text): pass
    def flush(self): pass
    def isatty(self): return False

if sys.stdout is None: sys.stdout = NullWriter()
if sys.stderr is None: sys.stderr = NullWriter()

# ---------------------- è·¯å¾„å¤„ç†è¾…åŠ©å‡½æ•° ----------------------
def get_resource_path(relative_path):
    """
    è·å–èµ„æºç»å¯¹è·¯å¾„ï¼Œå…¼å®¹å¼€å‘ç¯å¢ƒä¸ PyInstaller æ‰“åŒ…åçš„ç¯å¢ƒ
    """
    if getattr(sys, 'frozen', False):
        base_path_external = os.path.dirname(sys.executable)
    else:
        base_path_external = os.path.dirname(os.path.abspath(__file__))
    
    external_path = os.path.join(base_path_external, relative_path)
    if os.path.exists(external_path):
        return external_path

    if hasattr(sys, '_MEIPASS'):
        internal_path = os.path.join(sys._MEIPASS, relative_path)
        return internal_path

    return external_path

# ---------------------- æ ¸å¿ƒæ£€æµ‹çº¿ç¨‹ ----------------------
class AIGCDetectionThread(QThread):
    """
    ç‹¬ç«‹çš„åå°è¿ç®—çº¿ç¨‹ï¼Œé˜²æ­¢æ¨¡å‹æ¨ç†æ—¶å¡æ­»ä¸»ç•Œé¢
    """
    progress_signal = Signal(int)
    result_signal = Signal(dict)
    status_signal = Signal(str)
    device_signal = Signal(str, bool)

    def __init__(self, text, model_path):
        super().__init__()
        self.text = text
        self.model_path = model_path
        self.MIN_VALID_CHARS = 20
        self.TEMPERATURE = 2.0
        self.POWER_FACTOR = 3.5

    def calculate_human_features(self, text):
        """è®¡ç®—äººç±»å†™ä½œç‰¹å¾ï¼ˆå¥é•¿æ–¹å·®/çªå‘æ€§ï¼‰æ¥é™ä½è¯¯åˆ¤"""
        sentences = re.split(r'[ã€‚.!ï¼?ï¼Ÿ;ï¼›\n]+', text)
        sentences = [s for s in sentences if len(s.strip()) > 3]
        if len(sentences) < 3: return 0.0
        lengths = [len(s) for s in sentences]
        mean_len = sum(lengths) / len(lengths)
        variance = sum((l - mean_len) ** 2 for l in lengths) / len(lengths)
        std_dev = math.sqrt(variance)
        cv = std_dev / (mean_len + 1e-5)
        bonus = 0.0
        if cv > 0.4: bonus = min((cv - 0.4) * 0.6, 0.3)
        return bonus

    def run(self):
        if not self.model_path or not os.path.exists(self.model_path):
            self.result_signal.emit({"error": "æ¨¡å‹è·¯å¾„æ— æ•ˆ"})
            return

        try:
            # åœ¨çº¿ç¨‹å†…å»¶è¿Ÿå¯¼å…¥ä½“ç§¯åºå¤§çš„ AI åº“ï¼ŒåŠ å¿«è½¯ä»¶ç¬é—´å¯åŠ¨é€Ÿåº¦
            import torch
            from transformers import AutoModelForSequenceClassification, AutoTokenizer
            import torch.nn.functional as F

            # --- ç¡¬ä»¶æ£€æµ‹é€»è¾‘ ---
            use_cuda = torch.cuda.is_available()
            use_mps = hasattr(torch.backends, "mps") and torch.backends.mps.is_available()
            
            if use_cuda:
                device_str = "cuda"
                gpu_name = torch.cuda.get_device_name(0)
                if len(gpu_name) > 20: gpu_name = gpu_name[:20] + "..."
                self.device_signal.emit(f"ğŸš€ æ˜¾å¡åŠ é€Ÿ: {gpu_name} (Torch {torch.__version__})", True)
                torch_device = torch.device("cuda")
            elif use_mps:
                device_str = "mps"
                self.device_signal.emit(f"âš¡ Mac GPU åŠ é€Ÿ (Torch {torch.__version__})", True)
                torch_device = torch.device("mps")
            else:
                device_str = "cpu"
                version = torch.__version__
                extra_info = " [é”™è¯¯: å®‰è£…äº†CPUç‰ˆTorch]" if "+cpu" in version else (" [æœªå‘ç°NVIDIAæ˜¾å¡]" if not use_cuda else "")
                self.device_signal.emit(f"ğŸ¢ CPU è¿ç®— (Torch {version}){extra_info}", False)
                torch_device = torch.device("cpu")
            
            self.progress_signal.emit(10)
            self.status_signal.emit("åŠ è½½æœ¬åœ°æƒé‡ (config, bin, vocab)...")
            
            # åŠ è½½æ¨¡å‹
            tokenizer = AutoTokenizer.from_pretrained(self.model_path, local_files_only=True)
            model = AutoModelForSequenceClassification.from_pretrained(self.model_path, local_files_only=True)
            model.to(torch_device)
            model.eval() 
            self.progress_signal.emit(30)

            # è·å– AI æ ‡ç­¾ ID
            ai_label_id = 1 
            if hasattr(model.config, 'id2label') and model.config.id2label:
                for idx, label in model.config.id2label.items():
                    if any(x in str(label).lower() for x in ['fake', 'ai', 'chatgpt', 'generated', '1', 'label_1']):
                        ai_label_id = int(idx); break

            paragraphs = [p for p in self.text.split("\n") if p.strip()]
            if not paragraphs:
                self.result_signal.emit({"total_ai_rate": 0, "paragraphs": []}); return

            results = []
            total_weighted_score = 0; total_valid_weight = 0

            # é€æ®µæ¨ç†
            for idx, para in enumerate(paragraphs):
                self.status_signal.emit(f"æ·±åº¦æŒ‡çº¹åˆ†æä¸­... {idx+1}/{len(paragraphs)}")
                try:
                    inputs = tokenizer(para, return_tensors="pt", truncation=True, max_length=512)
                    inputs = {k: v.to(torch_device) for k, v in inputs.items()}
                    with torch.no_grad():
                        outputs = model(**inputs)
                        logits = outputs.logits
                        scaled_logits = logits / self.TEMPERATURE
                        probs = F.softmax(scaled_logits, dim=-1)
                        raw_ai_score = probs[0][ai_label_id].item()
                        
                        # åº”ç”¨ç‰¹å¾æ‰£åˆ†å’ŒæŒ‡æ•°æƒ©ç½š
                        human_bonus = self.calculate_human_features(para)
                        adjusted_score = max(0.0, raw_ai_score - human_bonus)
                        final_ai_score = math.pow(adjusted_score, self.POWER_FACTOR)
                        ai_rate = round(final_ai_score * 100, 2)
                    
                    # ç»Ÿè®¡æ±‰å­—æ•°é‡ (æ¶µç›–ç»å¤§å¤šæ•°å¸¸ç”¨ä¸­æ–‡å­—ç¬¦ï¼Œæ¯ä¸ªç®— 1 ä¸ªå­—ç¬¦)
                    chinese_count = len(re.findall(r'[\u4e00-\u9fa5]', para))
                    
                    # ç»Ÿè®¡è‹±æ–‡å­—æ¯å’Œæ•°å­—æ•°é‡ (æ¯ä¸ªç®— 0.5 ä¸ªå­—ç¬¦)
                    alnum_count = len(re.findall(r'[a-zA-Z0-9]', para))
                    
                    # è®¡ç®—ç­‰æ•ˆé•¿åº¦ (æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼è¢«è‡ªåŠ¨å¿½ç•¥)
                    para_len = chinese_count + (alnum_count * 0.5)
                    
                    is_ignored = para_len < self.MIN_VALID_CHARS
                    weight = 0 if is_ignored else para_len
                    
                    results.append({"content": para, "ai_rate": ai_rate, "is_ignored": is_ignored})
                    if not is_ignored:
                        total_weighted_score += (ai_rate * weight); total_valid_weight += weight
                except Exception as e:
                    if "upgrade torch" in str(e) and "v2.6" in str(e): raise e 
                    print(f"Segment Error: {e}")
                
                self.progress_signal.emit(30 + int(((idx + 1) / len(paragraphs)) * 65))

            avg = round(total_weighted_score / total_valid_weight, 2) if total_valid_weight > 0 else 0
            self.result_signal.emit({"total_ai_rate": avg, "paragraphs": results})

        except Exception as e:
            if "upgrade torch" in str(e) and "v2.6" in str(e):
                self.result_signal.emit({"error": "ã€ç¯å¢ƒç‰ˆæœ¬å†²çªã€‘\nè¯·å‡çº§ PyTorch ç‰ˆæœ¬ã€‚\npip install --upgrade torch torchvision torchaudio"})
            else:
                self.result_signal.emit({"error": f"æ¨ç†å¼•æ“å¼‚å¸¸:\n{str(e)}"})